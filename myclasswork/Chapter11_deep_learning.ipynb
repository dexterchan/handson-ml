{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "import tensorflow as tf\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"deep\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = b\"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing/Exploding Gradients Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logit(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAERCAYAAACdPxtnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcca70d1240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [1, 1], 'k--')\n",
    "plt.plot([0, 0], [-0.2, 1.2], 'k-')\n",
    "plt.plot([-5, 5], [-3/4, 7/4], 'g--')\n",
    "plt.plot(z, logit(z), \"b-\", linewidth=2)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Saturating', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Saturating', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Sigmoid activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "\n",
    "#save_fig(\"sigmoid_saturation_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xavier and He\n",
    "Note: It is now preferable to use tf.layers.dense(), because anything in the contrib module may change or be deleted without notice. The dense() function is almost identical to the fully_connected() function. The main differences relevant to this chapter are:\n",
    "\n",
    "several parameters are renamed: scope becomes name, activation_fn becomes activation (and similarly the _fn suffix is removed from other parameters such as normalizer_fn), weights_initializer becomes kernel_initializer, etc.\n",
    "the default activation is now None rather than tf.nn.relu.\n",
    "it does not support tensorflow.contrib.framework.arg_scope() (introduced later in chapter 11).\n",
    "it does not support regularizer params (introduced later in chapter 11)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                          kernel_initializer=he_init, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    Q=sess.run(hidden1, feed_dict={X:np.random.random(size=(1,28*28))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonsaturating Activation Functions\n",
    "Leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.maximum(alpha*z, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'z' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-88cce65798e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"b-\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'k-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'k-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfacecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'black'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshrink\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'z' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
    "plt.grid(True)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.title(\"Leaky ReLU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this activation function, within 10 iteration, the neuron dies out with zero std deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: 0.20 < mean < 0.64, 0.35 < std deviation < 0.82\n",
      "Layer 10: 0.00 < mean < 0.03, 0.01 < std deviation < 0.04\n",
      "Layer 20: 0.00 < mean < 0.00, 0.00 < std deviation < 0.00\n",
      "Layer 30: 0.00 < mean < 0.00, 0.00 < std deviation < 0.00\n",
      "Layer 40: 0.00 < mean < 0.00, 0.00 < std deviation < 0.00\n",
      "Layer 50: 0.00 < mean < 0.00, 0.00 < std deviation < 0.00\n",
      "Layer 60: 0.00 < mean < 0.00, 0.00 < std deviation < 0.00\n",
      "Layer 70: 0.00 < mean < 0.00, 0.00 < std deviation < 0.00\n",
      "Layer 80: 0.00 < mean < 0.00, 0.00 < std deviation < 0.00\n",
      "Layer 90: 0.00 < mean < 0.00, 0.00 < std deviation < 0.00\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "np.random.seed(np.int32(time.time()))\n",
    "Z = np.random.normal(size=(500, 100))\n",
    "for layer in range(100):\n",
    "    W = np.random.normal(size=(100, 100), scale=np.sqrt(2/200))\n",
    "    Z = leaky_relu(np.dot(Z, W))\n",
    "    means = np.mean(Z, axis=1)\n",
    "    stds = np.std(Z, axis=1)\n",
    "    if layer % 10 == 0:\n",
    "        print(\"Layer {}: {:.2f} < mean < {:.2f}, {:.2f} < std deviation < {:.2f}\".format(\n",
    "            layer, means.min(), means.max(), stds.min(), stds.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing Leaky ReLU in TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z, name=None):\n",
    "    return tf.maximum(0.01 * z, z, name=name)\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a neural network on MNIST using the Leaky ReLU. First let's create the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=leaky_relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    lr = tf.argmax(logits,1)\n",
    "    correct2=tf.equal(lr,y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct2, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image, shape=[28, 28]):\n",
    "    plt.imshow(image.reshape(shape), cmap=\"Greys\", interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f85c67ea1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_batch, y_batch = mnist.train.next_batch(100)\n",
    "plot_image(X_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show label distribution statistics\n",
    "countY = np.bincount(y_batch.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11, 18, 10,  6, 11,  9,  5, 12,  9,  9])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=np.reshape(np.nonzero(countY),(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 11),\n",
       " (1, 18),\n",
       " (2, 10),\n",
       " (3, 6),\n",
       " (4, 11),\n",
       " (5, 9),\n",
       " (6, 5),\n",
       " (7, 12),\n",
       " (8, 9),\n",
       " (9, 9)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(x,y) for x,y in zip(index,countY[index]) ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "    sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "    logits_,y_,correct_,lr_,correct2_ = sess.run( (logits,y,correct,lr,correct2), feed_dict={X: X_batch, y: y_batch})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 3, 6, 0, 1, 1, 6, 0, 0])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(logits_,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 3, 6, 0, 1, 1, 6, 0, 0])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 7, 1, 0, 8, 1, 6, 0, 5, 1])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False,  True, False, False, False, False], dtype=bool)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False,  True, False, False, False, False], dtype=bool)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct2_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False,  True, False, False, False, False], dtype=bool)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.equal(np.argmax(logits_,1),y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 0.96 Validation accuracy: 0.9004\n",
      "5 Batch accuracy: 0.96 Validation accuracy: 0.95\n",
      "10 Batch accuracy: 0.94 Validation accuracy: 0.9626\n",
      "15 Batch accuracy: 0.94 Validation accuracy: 0.9716\n",
      "20 Batch accuracy: 0.98 Validation accuracy: 0.9752\n",
      "25 Batch accuracy: 0.98 Validation accuracy: 0.977\n",
      "30 Batch accuracy: 1.0 Validation accuracy: 0.978\n",
      "35 Batch accuracy: 0.98 Validation accuracy: 0.9798\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_test = accuracy.eval(feed_dict={X: mnist.validation.images, y: mnist.validation.labels})\n",
    "            print(epoch, \"Batch accuracy:\", acc_train, \"Validation accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def elu(z, alpha=1):\n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAETCAYAAADJUJaPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcca72e01d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, elu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1, -1], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"ELU activation function ($\\alpha=1$)\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this activation function, after a 10 layer deep neural network, neuron dies out with zero deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: -0.07 < mean < 0.43, 0.53 < std deviation < 1.01\n",
      "Layer 10: -0.04 < mean < 0.11, 0.11 < std deviation < 0.32\n",
      "Layer 20: -0.02 < mean < 0.04, 0.07 < std deviation < 0.18\n",
      "Layer 30: -0.01 < mean < 0.01, 0.03 < std deviation < 0.10\n",
      "Layer 40: -0.00 < mean < 0.01, 0.02 < std deviation < 0.08\n",
      "Layer 50: -0.01 < mean < 0.01, 0.01 < std deviation < 0.07\n",
      "Layer 60: -0.00 < mean < 0.00, 0.01 < std deviation < 0.04\n",
      "Layer 70: -0.00 < mean < 0.00, 0.01 < std deviation < 0.04\n",
      "Layer 80: -0.00 < mean < 0.00, 0.00 < std deviation < 0.04\n",
      "Layer 90: -0.00 < mean < 0.00, 0.00 < std deviation < 0.04\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "np.random.seed(np.int32(time.time()))\n",
    "Z = np.random.normal(size=(500, 100))\n",
    "for layer in range(100):\n",
    "    W = np.random.normal(size=(100, 100), scale=np.sqrt(2/200))\n",
    "    Z = elu(np.dot(Z, W))\n",
    "    means = np.mean(Z, axis=1)\n",
    "    stds = np.std(Z, axis=1)\n",
    "    if layer % 10 == 0:\n",
    "        print(\"Layer {}: {:.2f} < mean < {:.2f}, {:.2f} < std deviation < {:.2f}\".format(\n",
    "            layer, means.min(), means.max(), stds.min(), stds.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing ELU in TensorFlow is trivial, just specify the activation function when building each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.elu, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    Q=sess.run(hidden1, feed_dict={X:np.random.random(size=(1,28*28))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This activation function was proposed in this https://arxiv.org/pdf/1706.02515.pdf by Günter Klambauer, Thomas Unterthiner and Andreas Mayr, published in June 2017. It outperforms the other activation functions very significantly for deep neural networks, so you should really try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def elu(z, alpha=1):\n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)\n",
    "def selu(z,\n",
    "         scale=1.0507009873554804934193349852946,\n",
    "         alpha=1.6732632423543772848170429916717):\n",
    "    return scale * elu(z, alpha)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAERCAYAAACEmDeEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcca72a5a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reset_graph()\n",
    "z = np.linspace(-5, 5, 200)\n",
    "#plt.plot(z, selu(z,0.05), \"b-\", linewidth=2)\n",
    "plt.plot(z, selu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1.758, -1.758], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"SELU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this activation function, even a 100 layer deep neural network preserves roughly mean 0 and standard deviation 1 across all layers, avoiding the exploding/vanishing gradients problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: -0.32 < mean < 0.34, 0.72 < std deviation < 1.23\n",
      "Layer 10: -0.33 < mean < 0.33, 0.70 < std deviation < 1.26\n",
      "Layer 20: -0.19 < mean < 0.27, 0.82 < std deviation < 1.40\n",
      "Layer 30: -0.26 < mean < 0.25, 0.66 < std deviation < 1.17\n",
      "Layer 40: -0.16 < mean < 0.12, 0.71 < std deviation < 1.23\n",
      "Layer 50: -0.26 < mean < 0.30, 0.69 < std deviation < 1.25\n",
      "Layer 60: -0.19 < mean < 0.17, 0.75 < std deviation < 1.30\n",
      "Layer 70: -0.24 < mean < 0.23, 0.73 < std deviation < 1.26\n",
      "Layer 80: -0.28 < mean < 0.23, 0.74 < std deviation < 1.20\n",
      "Layer 90: -0.18 < mean < 0.13, 0.74 < std deviation < 1.19\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "np.random.seed(np.int32(time.time()))\n",
    "Z = np.random.normal(size=(500, 100))\n",
    "for layer in range(100):\n",
    "    W = np.random.normal(size=(100, 100), scale=np.sqrt(2/200))\n",
    "    Z = selu(np.dot(Z, W))\n",
    "    means = np.mean(Z, axis=1)\n",
    "    stds = np.std(Z, axis=1)\n",
    "    if layer % 10 == 0:\n",
    "        print(\"Layer {}: {:.2f} < mean < {:.2f}, {:.2f} < std deviation < {:.2f}\".format(\n",
    "            layer, means.min(), means.max(), stds.min(), stds.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def selu(z,\n",
    "         scale=1.0507009873554804934193349852946,\n",
    "         alpha=1.6732632423543772848170429916717):\n",
    "    return scale * tf.where(z >= 0.0, z, alpha * tf.nn.elu(z))\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "\n",
    "    \n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=selu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=selu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "n_epochs = 40\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 0.96 Validation accuracy: 0.924\n",
      "5 Batch accuracy: 1.0 Validation accuracy: 0.9568\n",
      "10 Batch accuracy: 0.94 Validation accuracy: 0.967\n",
      "15 Batch accuracy: 0.98 Validation accuracy: 0.9684\n",
      "20 Batch accuracy: 1.0 Validation accuracy: 0.9712\n",
      "25 Batch accuracy: 1.0 Validation accuracy: 0.9692\n",
      "30 Batch accuracy: 1.0 Validation accuracy: 0.9698\n",
      "35 Batch accuracy: 1.0 Validation accuracy: 0.971\n"
     ]
    }
   ],
   "source": [
    "means = mnist.train.images.mean(axis=0, keepdims=True)\n",
    "stds = mnist.train.images.std(axis=0, keepdims=True) + 1e-10\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            X_batch_scaled = (X_batch - means) / stds\n",
    "            sess.run(training_op, feed_dict={X: X_batch_scaled, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_train = accuracy.eval(feed_dict={X: X_batch_scaled, y: y_batch})\n",
    "            X_val_scaled = (mnist.validation.images - means) / stds\n",
    "            acc_test = accuracy.eval(feed_dict={X: X_val_scaled, y: mnist.validation.labels})\n",
    "            print(epoch, \"Batch accuracy:\", acc_train, \"Validation accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final_selu.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch normalization\n",
    "Adding input data normalization of shift and scale at each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "    bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n",
    "    bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "    hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "    bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\n",
    "    bn2_act = tf.nn.elu(bn2)\n",
    "\n",
    "    logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "    logits = tf.layers.batch_normalization(logits_before_bn, training=training,\n",
    "                                           momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid repeating the same parameters over and over again, we can use Python's partial() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    my_batch_norm_layer = partial(tf.layers.batch_normalization,\n",
    "                              training=training, momentum=0.9)\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "    bn1 = my_batch_norm_layer(hidden1)\n",
    "    bn1_act = tf.nn.elu(bn1)\n",
    "    hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "    bn2 = my_batch_norm_layer(hidden2)\n",
    "    bn2_act = tf.nn.elu(bn2)\n",
    "    logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "    logits = my_batch_norm_layer(logits_before_bn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a neural net for MNIST, using the ELU activation function and Batch Normalization at each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "batch_norm_momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "    my_batch_norm_layer = partial(\n",
    "            tf.layers.batch_normalization,\n",
    "            training=training,\n",
    "            momentum=batch_norm_momentum)\n",
    "\n",
    "    my_dense_layer = partial(\n",
    "            tf.layers.dense,\n",
    "            kernel_initializer=he_init)\n",
    "\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    bn1 = tf.nn.elu(my_batch_norm_layer(hidden1))\n",
    "    hidden2 = my_dense_layer(bn1, n_hidden2, name=\"hidden2\")\n",
    "    bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))\n",
    "    logits_before_bn = my_dense_layer(bn2, n_outputs, name=\"outputs\")\n",
    "    logits = my_batch_norm_layer(logits_before_bn)\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: since we are using tf.layers.batch_normalization() rather than tf.contrib.layers.batch_norm(), we need to explicitly run the extra update operations needed by batch normalization (sess.run([training_op, extra_update_ops],...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.8652\n",
      "1 Test accuracy: 0.8961\n",
      "2 Test accuracy: 0.9122\n",
      "3 Test accuracy: 0.9222\n",
      "4 Test accuracy: 0.929\n",
      "5 Test accuracy: 0.9351\n",
      "6 Test accuracy: 0.9379\n",
      "7 Test accuracy: 0.9435\n",
      "8 Test accuracy: 0.9451\n",
      "9 Test accuracy: 0.9485\n",
      "10 Test accuracy: 0.951\n",
      "11 Test accuracy: 0.9532\n",
      "12 Test accuracy: 0.9555\n",
      "13 Test accuracy: 0.9576\n",
      "14 Test accuracy: 0.9597\n",
      "15 Test accuracy: 0.9609\n",
      "16 Test accuracy: 0.9615\n",
      "17 Test accuracy: 0.9634\n",
      "18 Test accuracy: 0.9641\n",
      "19 Test accuracy: 0.965\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "file_writer = tf.summary.FileWriter(\"./model/batch_norm\", tf.get_default_graph())\n",
    "file_writer.close()\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run([training_op, extra_update_ops],\n",
    "                     feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "    save_path = saver.save(sess, \"./model/my_model_deepLearning_batch_norm_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hidden1/kernel:0',\n",
       " 'hidden1/bias:0',\n",
       " 'batch_normalization/gamma:0',\n",
       " 'batch_normalization/beta:0',\n",
       " 'hidden2/kernel:0',\n",
       " 'hidden2/bias:0',\n",
       " 'batch_normalization_1/gamma:0',\n",
       " 'batch_normalization_1/beta:0',\n",
       " 'outputs/kernel:0',\n",
       " 'outputs/bias:0',\n",
       " 'batch_normalization_2/gamma:0',\n",
       " 'batch_normalization_2/beta:0']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tf.trainable_variables()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hidden1/kernel:0',\n",
       " 'hidden1/bias:0',\n",
       " 'batch_normalization/gamma:0',\n",
       " 'batch_normalization/beta:0',\n",
       " 'batch_normalization/moving_mean:0',\n",
       " 'batch_normalization/moving_variance:0',\n",
       " 'hidden2/kernel:0',\n",
       " 'hidden2/bias:0',\n",
       " 'batch_normalization_1/gamma:0',\n",
       " 'batch_normalization_1/beta:0',\n",
       " 'batch_normalization_1/moving_mean:0',\n",
       " 'batch_normalization_1/moving_variance:0',\n",
       " 'outputs/kernel:0',\n",
       " 'outputs/bias:0',\n",
       " 'batch_normalization_2/gamma:0',\n",
       " 'batch_normalization_2/beta:0',\n",
       " 'batch_normalization_2/moving_mean:0',\n",
       " 'batch_normalization_2/moving_variance:0']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tf.global_variables()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Reusing Pretrained Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reusing a TensorFlow Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First you need to load the graph's structure. The import_meta_graph() function does just that, loading the graph's operations into the default graph, and returning a Saver that you can then use to restore the model's state. Note that by default, a Saver saves the structure of the graph into a .meta file, so that's the file you should load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.import_meta_graph(\"./model/my_model_deepLearning_batch_norm_final.ckpt.meta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "training = tf.get_default_graph().get_tensor_by_name(\"training:0\")\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"eval/Mean:0\")\n",
    "\n",
    "training_op = tf.get_default_graph().get_operation_by_name(\"train/GradientDescent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for op in (X, y, training, accuracy, training_op):\n",
    "    tf.add_to_collection(\"my_important_ops2\", op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way people who reuse your model will be able to simply write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, training, accuracy, training_op = tf.get_collection(\"my_important_ops2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can start a session, restore the model's state and continue training on your data:\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./model/my_model_deepLearning_batch_norm_final.ckpt\")\n",
    "    # continue training the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/my_model_deepLearning_batch_norm_final.ckpt\n",
      "[2018-02-19 03:26:22,574] {tf_logging.py:82} INFO - Restoring parameters from ./model/my_model_deepLearning_batch_norm_final.ckpt\n",
      "0 Test accuracy: 0.9657\n",
      "1 Test accuracy: 0.9661\n",
      "2 Test accuracy: 0.9674\n",
      "3 Test accuracy: 0.9672\n",
      "4 Test accuracy: 0.9682\n",
      "5 Test accuracy: 0.9682\n",
      "6 Test accuracy: 0.9685\n",
      "7 Test accuracy: 0.9688\n",
      "8 Test accuracy: 0.9692\n",
      "9 Test accuracy: 0.9691\n",
      "10 Test accuracy: 0.9695\n",
      "11 Test accuracy: 0.9688\n",
      "12 Test accuracy: 0.9688\n",
      "13 Test accuracy: 0.9696\n",
      "14 Test accuracy: 0.9695\n",
      "15 Test accuracy: 0.9695\n",
      "16 Test accuracy: 0.9689\n",
      "17 Test accuracy: 0.9689\n",
      "18 Test accuracy: 0.9693\n",
      "19 Test accuracy: 0.9695\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "acc_summary = tf.summary.scalar('Accuracy', accuracy)\n",
    "\n",
    "file_writer = tf.summary.FileWriter(\"./model_reuse/batch_norm\", tf.get_default_graph())\n",
    "\n",
    "#saver = tf.train.import_meta_graph(\"./model/my_model_deepLearning_batch_norm_final.ckpt.meta\")\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./model/my_model_deepLearning_batch_norm_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        \n",
    "        summary_str = acc_summary.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        file_writer.add_summary(summary_str, epoch)\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./model_reuse/my_model_deepLearning_batch_norm_final.ckpt\")  \n",
    "    file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the most simple one \"my_model_final\"\n",
    "\n",
    "In general you will want to reuse only the lower layers. If you are using import_meta_graph() it will load the whole graph, but you can simply ignore the parts you do not need. In this example, we add a new 4th hidden layer on top of the pretrained 3rd layer (ignoring the old 4th hidden layer). We also build a new output layer, the loss for this new output, and a new optimizer to minimize it. We also need another saver to save the whole graph (containing both the entire old graph plus the new operations), and an initialization operation to initialize all the new variables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-define the model first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z, name=None):\n",
    "    return tf.maximum(0.01 * z, z, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "0 Batch accuracy: 0.9 Validation accuracy: 0.9032\n",
      "5 Batch accuracy: 0.88 Validation accuracy: 0.9504\n",
      "10 Batch accuracy: 0.98 Validation accuracy: 0.964\n",
      "15 Batch accuracy: 1.0 Validation accuracy: 0.971\n",
      "20 Batch accuracy: 1.0 Validation accuracy: 0.976\n",
      "25 Batch accuracy: 1.0 Validation accuracy: 0.9772\n",
      "30 Batch accuracy: 1.0 Validation accuracy: 0.978\n",
      "35 Batch accuracy: 0.98 Validation accuracy: 0.9796\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=leaky_relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_test = accuracy.eval(feed_dict={X: mnist.validation.images, y: mnist.validation.labels})\n",
    "            print(epoch, \"Batch accuracy:\", acc_train, \"Validation accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a new model\n",
    "# Freezing a lower layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "[2018-02-19 03:34:15,578] {tf_logging.py:82} INFO - Restoring parameters from ./my_model_final.ckpt\n",
      "0 Test accuracy: 0.9617\n",
      "1 Test accuracy: 0.9675\n",
      "2 Test accuracy: 0.9705\n",
      "3 Test accuracy: 0.9721\n",
      "4 Test accuracy: 0.9733\n",
      "5 Test accuracy: 0.9742\n",
      "6 Test accuracy: 0.9741\n",
      "7 Test accuracy: 0.9743\n",
      "8 Test accuracy: 0.9734\n",
      "9 Test accuracy: 0.9745\n",
      "10 Test accuracy: 0.9755\n",
      "11 Test accuracy: 0.9747\n",
      "12 Test accuracy: 0.9741\n",
      "13 Test accuracy: 0.9749\n",
      "14 Test accuracy: 0.9757\n",
      "15 Test accuracy: 0.9753\n",
      "16 Test accuracy: 0.9755\n",
      "17 Test accuracy: 0.9755\n",
      "18 Test accuracy: 0.9744\n",
      "19 Test accuracy: 0.9754\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # new!\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "learning_rate = 0.01\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")       # reused\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\") # reused\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\") # reused\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")                         # new!\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "    \n",
    "with tf.name_scope(\"train\"):                                         # not shown in the book\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)     # not shown\n",
    "    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                   scope=\"hidden[34]|outputs\")\n",
    "    training_op = optimizer.minimize(loss, var_list=train_vars)\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "new_saver = tf.train.Saver()\n",
    "\n",
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"hidden[12]\") # regular expression\n",
    "reuse_vars_dict = dict([(var.op.name, var) for var in reuse_vars])\n",
    "restore_saver = tf.train.Saver(reuse_vars_dict) # to restore layers 1-2\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_new_model_final.ckpt\n",
      "[2018-02-19 03:35:10,818] {tf_logging.py:82} INFO - Restoring parameters from ./my_new_model_final.ckpt\n",
      "0 Test accuracy: 0.9714\n",
      "1 Test accuracy: 0.974\n",
      "2 Test accuracy: 0.9752\n",
      "3 Test accuracy: 0.976\n",
      "4 Test accuracy: 0.9757\n",
      "5 Test accuracy: 0.9743\n",
      "6 Test accuracy: 0.9747\n",
      "7 Test accuracy: 0.9755\n",
      "8 Test accuracy: 0.9756\n",
      "9 Test accuracy: 0.9759\n",
      "10 Test accuracy: 0.9754\n",
      "11 Test accuracy: 0.977\n",
      "12 Test accuracy: 0.9765\n",
      "13 Test accuracy: 0.9761\n",
      "14 Test accuracy: 0.9759\n",
      "15 Test accuracy: 0.976\n",
      "16 Test accuracy: 0.9756\n",
      "17 Test accuracy: 0.9759\n",
      "18 Test accuracy: 0.9762\n",
      "19 Test accuracy: 0.9755\n"
     ]
    }
   ],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"hidden[123]\") # regular expression\n",
    "reuse_vars_dict = dict([(var.op.name, var) for var in reuse_vars])\n",
    "restore_saver = tf.train.Saver(reuse_vars_dict) # to restore layers 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_new_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching the frozen layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_new_model_final.ckpt\n",
      "[2018-02-19 03:36:07,152] {tf_logging.py:82} INFO - Restoring parameters from ./my_new_model_final.ckpt\n",
      "0 Test accuracy: 0.9691\n",
      "1 Test accuracy: 0.9717\n",
      "2 Test accuracy: 0.9741\n",
      "3 Test accuracy: 0.9738\n",
      "4 Test accuracy: 0.9745\n",
      "5 Test accuracy: 0.9752\n",
      "6 Test accuracy: 0.9755\n",
      "7 Test accuracy: 0.9753\n",
      "8 Test accuracy: 0.9758\n",
      "9 Test accuracy: 0.9753\n",
      "10 Test accuracy: 0.9761\n",
      "11 Test accuracy: 0.9762\n",
      "12 Test accuracy: 0.9756\n",
      "13 Test accuracy: 0.9774\n",
      "14 Test accuracy: 0.9772\n",
      "15 Test accuracy: 0.9752\n",
      "16 Test accuracy: 0.9769\n",
      "17 Test accuracy: 0.9767\n",
      "18 Test accuracy: 0.9766\n",
      "19 Test accuracy: 0.9765\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                              name=\"hidden1\") # reused frozen\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
    "                              name=\"hidden2\") # reused frozen & cached\n",
    "    hidden2_stop = tf.stop_gradient(hidden2)\n",
    "    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu,\n",
    "                              name=\"hidden3\") # reused, not frozen\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu,\n",
    "                              name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\") # new!\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"hidden[123]\") # regular expression\n",
    "reuse_vars_dict = dict([(var.op.name, var) for var in reuse_vars])\n",
    "restore_saver = tf.train.Saver(reuse_vars_dict) # to restore layers 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "import numpy as np\n",
    "\n",
    "n_batches = mnist.train.num_examples // batch_size\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_new_model_final.ckpt\")\n",
    "    \n",
    "    h2_cache = sess.run(hidden2, feed_dict={X: mnist.train.images})\n",
    "    h2_cache_test = sess.run(hidden2, feed_dict={X: mnist.test.images}) # not shown in the book\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        shuffled_idx = np.random.permutation(mnist.train.num_examples)\n",
    "        hidden2_batches = np.array_split(h2_cache[shuffled_idx], n_batches)\n",
    "        y_batches = np.array_split(mnist.train.labels[shuffled_idx], n_batches)\n",
    "        for hidden2_batch, y_batch in zip(hidden2_batches, y_batches):\n",
    "            sess.run(training_op, feed_dict={hidden2:hidden2_batch, y:y_batch})\n",
    "\n",
    "        accuracy_val = accuracy.eval(feed_dict={hidden2: h2_cache_test, # not shown\n",
    "                                                y: mnist.test.labels})  # not shown\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)                    # not shown\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final2.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avoiding Overfitting Through Regularization\n",
    "ℓ1ℓ1  and ℓ2ℓ2 regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement  ℓ1ℓ1  regularization manually. First, we create the model, as usual (with just one hidden layer this time, for simplicity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    logits = tf.layers.dense(hidden1, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we get a handle on the layer weights, and we compute the total loss, which is equal to the sum of the usual cross entropy loss and the  ℓ1ℓ1  loss (i.e., the absolute values of the weights):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W1 = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
    "W2 = tf.get_default_graph().get_tensor_by_name(\"outputs/kernel:0\")\n",
    "\n",
    "scale = 0.001 # l1 regularization hyperparameter\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                              logits=logits)\n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")\n",
    "    reg_losses = tf.reduce_sum(tf.abs(W1)) + tf.reduce_sum(tf.abs(W2))\n",
    "    loss = tf.add(base_loss, scale * reg_losses, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.8348\n",
      "1 Test accuracy: 0.8702\n",
      "2 Test accuracy: 0.8814\n",
      "3 Test accuracy: 0.8905\n",
      "4 Test accuracy: 0.8964\n",
      "5 Test accuracy: 0.8983\n",
      "6 Test accuracy: 0.9006\n",
      "7 Test accuracy: 0.903\n",
      "8 Test accuracy: 0.9041\n",
      "9 Test accuracy: 0.905\n",
      "10 Test accuracy: 0.9063\n",
      "11 Test accuracy: 0.9068\n",
      "12 Test accuracy: 0.9062\n",
      "13 Test accuracy: 0.9075\n",
      "14 Test accuracy: 0.9078\n",
      "15 Test accuracy: 0.9079\n",
      "16 Test accuracy: 0.9069\n",
      "17 Test accuracy: 0.9067\n",
      "18 Test accuracy: 0.9059\n",
      "19 Test accuracy: 0.9063\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_reg.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can pass a regularization function to the tf.layers.dense() function, which will use it to create operations that will compute the regularization loss, and it adds these operations to the collection of regularization losses. The beginning is the same as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.8293\n",
      "1 Test accuracy: 0.878\n",
      "2 Test accuracy: 0.8924\n",
      "3 Test accuracy: 0.9014\n",
      "4 Test accuracy: 0.9061\n",
      "5 Test accuracy: 0.91\n",
      "6 Test accuracy: 0.913\n",
      "7 Test accuracy: 0.9149\n",
      "8 Test accuracy: 0.9158\n",
      "9 Test accuracy: 0.9171\n",
      "10 Test accuracy: 0.9171\n",
      "11 Test accuracy: 0.918\n",
      "12 Test accuracy: 0.9183\n",
      "13 Test accuracy: 0.9197\n",
      "14 Test accuracy: 0.9183\n",
      "15 Test accuracy: 0.9191\n",
      "16 Test accuracy: 0.9189\n",
      "17 Test accuracy: 0.9177\n",
      "18 Test accuracy: 0.919\n",
      "19 Test accuracy: 0.918\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "scale = 0.001 # l1 regularization hyperparameter\n",
    "\n",
    "my_dense_layer = partial(\n",
    "    tf.layers.dense, activation=tf.nn.relu,\n",
    "    kernel_regularizer=tf.contrib.layers.l1_regularizer(scale))\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    hidden2 = my_dense_layer(hidden1, n_hidden2, name=\"hidden2\")\n",
    "    logits = my_dense_layer(hidden2, n_outputs, activation=None,\n",
    "                            name=\"outputs\")\n",
    "#Next we must add the regularization losses to the base loss:\n",
    "with tf.name_scope(\"loss\"):                                     # not shown in the book\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(  # not shown\n",
    "        labels=y, logits=logits)                                # not shown\n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")   # not shown\n",
    "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    loss = tf.add_n([base_loss] + reg_losses, name=\"loss\")\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_reg2.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.9237\n",
      "1 Test accuracy: 0.9409\n",
      "2 Test accuracy: 0.9486\n",
      "3 Test accuracy: 0.9521\n",
      "4 Test accuracy: 0.9559\n",
      "5 Test accuracy: 0.9584\n",
      "6 Test accuracy: 0.9593\n",
      "7 Test accuracy: 0.9575\n",
      "8 Test accuracy: 0.961\n",
      "9 Test accuracy: 0.9619\n",
      "10 Test accuracy: 0.9637\n",
      "11 Test accuracy: 0.9636\n",
      "12 Test accuracy: 0.9688\n",
      "13 Test accuracy: 0.9667\n",
      "14 Test accuracy: 0.9675\n",
      "15 Test accuracy: 0.9685\n",
      "16 Test accuracy: 0.9686\n",
      "17 Test accuracy: 0.9697\n",
      "18 Test accuracy: 0.9705\n",
      "19 Test accuracy: 0.9694\n"
     ]
    }
   ],
   "source": [
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "dropout_rate = 0.5  # == 1 - keep_prob\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
    "learning_rate = 0.01\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu,\n",
    "                              name=\"hidden1\")\n",
    "    hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)\n",
    "    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation=tf.nn.relu,\n",
    "                              name=\"hidden2\")\n",
    "    hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\n",
    "    logits = tf.layers.dense(hidden2_drop, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.9559\n",
      "1 Test accuracy: 0.9676\n",
      "2 Test accuracy: 0.9732\n",
      "3 Test accuracy: 0.9742\n",
      "4 Test accuracy: 0.974\n",
      "5 Test accuracy: 0.9775\n",
      "6 Test accuracy: 0.9801\n",
      "7 Test accuracy: 0.9791\n",
      "8 Test accuracy: 0.9755\n",
      "9 Test accuracy: 0.9817\n",
      "10 Test accuracy: 0.9803\n",
      "11 Test accuracy: 0.977\n",
      "12 Test accuracy: 0.9802\n",
      "13 Test accuracy: 0.9814\n",
      "14 Test accuracy: 0.9811\n",
      "15 Test accuracy: 0.9827\n",
      "16 Test accuracy: 0.9824\n",
      "17 Test accuracy: 0.983\n",
      "18 Test accuracy: 0.9826\n",
      "19 Test accuracy: 0.9832\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "threshold = 1.0\n",
    "weights = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
    "clipped_weights = tf.clip_by_norm(weights, clip_norm=threshold, axes=1)\n",
    "clip_weights = tf.assign(weights, clipped_weights)\n",
    "\n",
    "weights2 = tf.get_default_graph().get_tensor_by_name(\"hidden2/kernel:0\")\n",
    "clipped_weights2 = tf.clip_by_norm(weights2, clip_norm=threshold, axes=1)\n",
    "clip_weights2 = tf.assign(weights2, clipped_weights2)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:                                              # not shown in the book\n",
    "    init.run()                                                          # not shown\n",
    "    for epoch in range(n_epochs):                                       # not shown\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):  # not shown\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)       # not shown\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            clip_weights.eval()\n",
    "            clip_weights2.eval()                                        # not shown\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images,       # not shown\n",
    "                                            y: mnist.test.labels})      # not shown\n",
    "        print(epoch, \"Test accuracy:\", acc_test)                        # not shown\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")               # not shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_graph(tf.get_default_graph())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation above is straightforward and it works fine, but it is a bit messy. A better approach is to define a max_norm_regularizer() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_norm_regularizer(threshold, axes=1, name=\"max_norm\",\n",
    "                         collection=\"max_norm\"):\n",
    "    def max_norm(weights):\n",
    "        clipped = tf.clip_by_norm(weights, clip_norm=threshold, axes=axes)\n",
    "        clip_weights = tf.assign(weights, clipped, name=name)\n",
    "        tf.add_to_collection(collection, clip_weights)\n",
    "        return None # there is no regularization loss term\n",
    "    return max_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can call this function to get a max norm regularizer (with the threshold you want). When you create a hidden layer, you can pass this regularizer to the kernel_regularizer argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.9549\n",
      "1 Test accuracy: 0.971\n",
      "2 Test accuracy: 0.9702\n",
      "3 Test accuracy: 0.9656\n",
      "4 Test accuracy: 0.972\n",
      "5 Test accuracy: 0.9753\n",
      "6 Test accuracy: 0.976\n",
      "7 Test accuracy: 0.9777\n",
      "8 Test accuracy: 0.9783\n",
      "9 Test accuracy: 0.9782\n",
      "10 Test accuracy: 0.9811\n",
      "11 Test accuracy: 0.9776\n",
      "12 Test accuracy: 0.9808\n",
      "13 Test accuracy: 0.9801\n",
      "14 Test accuracy: 0.9816\n",
      "15 Test accuracy: 0.9813\n",
      "16 Test accuracy: 0.9804\n",
      "17 Test accuracy: 0.981\n",
      "18 Test accuracy: 0.9819\n",
      "19 Test accuracy: 0.9819\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "max_norm_reg = max_norm_regularizer(threshold=1.0)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                              kernel_regularizer=max_norm_reg, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
    "                              kernel_regularizer=max_norm_reg, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "clip_all_weights = tf.get_collection(\"max_norm\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            sess.run(clip_all_weights)\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images,     # not shown in the book\n",
    "                                            y: mnist.test.labels})    # not shown\n",
    "        print(epoch, \"Test accuracy:\", acc_test)                      # not shown\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")             # not shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
